{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from sklearn.metrics import  accuracy_score ,mean_squared_error , mean_absolute_error ,r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Linear Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading thee preprocessed data @sabinvankathmandu\n",
    "linear_model = joblib.load(\"../model/linear_regression_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading thee preprocessed data @sabinvankathmandu\n",
    "X_train = pd.read_pickle(\"../data/X_train.pkl\")\n",
    "X_test = pd.read_pickle(\"../data/X_test.pkl\")\n",
    "y_train = pd.read_pickle(\"../data/y_train.pkl\")\n",
    "y_test = pd.read_pickle(\"../data/y_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear model predictions\n",
    "y_train_pred = linear_model.predict(X_train)\n",
    "y_test_pred = linear_model.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Linear Regression\n",
    "mae_lr = mean_absolute_error(y_test, y_test_pred)\n",
    "mse_lr = mean_squared_error(y_test, y_test_pred)\n",
    "r2_lr = r2_score(y_test, y_test_pred)\n",
    "rmse = np.sqrt(mse_lr)\n",
    "\n",
    "print(f\"Linear Regression MAE {mae_lr:.4f}, MSE: {mse_lr:.4f}, R²: {r2_lr:.4f} RMSE {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize actual vs precited linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the actual vs predicted price linear plot @sabinvankathmandu\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=y_test, y=y_test_pred, alpha=0.7)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"--r\", lw=2)  # Identity line\n",
    "plt.xlabel(\"Actual Prices\")\n",
    "plt.ylabel(\"Predicted Prices\")\n",
    "plt.title(\"Actual vs Predicted House Prices\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading lasso and ridge models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved models\n",
    "ridge_best = joblib.load(\"../model/ridge_model.pkl\")\n",
    "lasso_best = joblib.load(\"../model/lasso_model.pkl\")\n",
    "\n",
    "print(\"Ridge and Lasso models loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "# Ridge Model Predictions\n",
    "y_pred_ridge = ridge_best.predict(X_test)\n",
    "\n",
    "# Lasso Model Predictions\n",
    "y_pred_lasso = lasso_best.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Ridge Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Ridge Model\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "\n",
    "# Evaluate Lasso Model\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "\n",
    "# Display Results\n",
    "print(f\"Ridge Model MSE: {mse_ridge:.4f}, R²: {r2_ridge:.4f}\")\n",
    "print(f\"Lasso Model MSE: {mse_lasso:.4f}, R²: {r2_lasso:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Actual vs Predicted Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Ridge Model: Actual vs Predicted\n",
    "sns.scatterplot(x=y_test, y=y_pred_ridge, label=\"Ridge\", alpha=0.7)\n",
    "\n",
    "# Lasso Model: Actual vs Predicted\n",
    "sns.scatterplot(x=y_test, y=y_pred_lasso, label=\"Lasso\", alpha=0.7)\n",
    "\n",
    "# Identity Line (for reference)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"--r\", lw=2)\n",
    "\n",
    "# Titles and labels\n",
    "plt.xlabel(\"Actual Prices\")\n",
    "plt.ylabel(\"Predicted Prices\")\n",
    "plt.title(\"Actual vs Predicted Prices (Ridge & Lasso)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Linear Regression Residuals\n",
    "sns.histplot(y_test - y_test_pred, bins=30, kde=True, color=\"blue\", label=\"Linear Regression\", alpha=0.6)\n",
    "\n",
    "# Ridge Residuals\n",
    "sns.histplot(y_test - y_pred_ridge, bins=30, kde=True, color=\"green\", label=\"Ridge\", alpha=0.6)\n",
    "\n",
    "# Lasso Residuals\n",
    "sns.histplot(y_test - y_pred_lasso, bins=30, kde=True, color=\"orange\", label=\"Lasso\", alpha=0.6)\n",
    "\n",
    "plt.xlabel(\"Residuals (Error)\")\n",
    "plt.title(\"Distribution of Residuals (Linear, Ridge & Lasso)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Ridge Residuals\n",
    "sns.histplot(y_test - y_pred_ridge, bins=30, kde=True, color=\"green\", label=\"Ridge Residuals\", alpha=0.6)\n",
    "\n",
    "# Lasso Residuals\n",
    "sns.histplot(y_test - y_pred_lasso, bins=30, kde=True, color=\"orange\", label=\"Lasso Residuals\", alpha=0.6)\n",
    "\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.title(\"Distribution of Residuals (Ridge & Lasso)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance (Ridge and Lasso Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Get coefficients for Ridge and Lasso models\n",
    "ridge_coefs = ridge_best.coef_\n",
    "lasso_coefs = lasso_best.coef_\n",
    "\n",
    "# Create DataFrame for easier comparison\n",
    "coefs_df = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Ridge Coefficients\": ridge_coefs,\n",
    "    \"Lasso Coefficients\": lasso_coefs\n",
    "})\n",
    "\n",
    "# Sort by absolute importance\n",
    "coefs_df['Ridge Importance'] = np.abs(coefs_df['Ridge Coefficients'])\n",
    "coefs_df['Lasso Importance'] = np.abs(coefs_df['Lasso Coefficients'])\n",
    "coefs_df_sorted = coefs_df.sort_values(by=\"Ridge Importance\", ascending=False)\n",
    "\n",
    "# Plot Feature Importance (Ridge & Lasso)\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=\"Ridge Importance\", y=\"Feature\", data=coefs_df_sorted, color=\"blue\", label=\"Ridge\", alpha=0.7)\n",
    "sns.barplot(x=\"Lasso Importance\", y=\"Feature\", data=coefs_df_sorted, color=\"orange\", label=\"Lasso\", alpha=0.7)\n",
    "plt.title(\"Feature Importance (Ridge & Lasso Models)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation metrics comparisons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Ridge and Lasso models\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "\n",
    "# Display Results\n",
    "results_df = pd.DataFrame({\n",
    "    \"Model\": [\"Linear Regression\", \"Ridge\", \"Lasso\"],\n",
    "    \"MSE\": [mse_lr, mse_ridge, mse_lasso],\n",
    "    \"R²\": [r2_lr, r2_ridge, r2_lasso]\n",
    "})\n",
    "\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save evaluatoins results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for the results\n",
    "results_df = pd.DataFrame({\n",
    "    \"Model\": [\"Ridge\", \"Lasso\"],\n",
    "    \"MSE\": [mse_ridge, mse_lasso],\n",
    "    \"R²\": [r2_ridge, r2_lasso]\n",
    "})\n",
    "\n",
    "# Save results to CSV\n",
    "results_df.to_csv(\"../model/ridge_lasso_evaluation_results.csv\", index=False)\n",
    "print(\"Evaluation results saved to CSV!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
